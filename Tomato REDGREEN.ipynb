{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83349f47-4166-4ad6-8b0f-a3f7290ef0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Resize(size=(224, 224))])\n",
    "from torchvision import datasets\n",
    "BASE_PATH='/home/bubbles/Quantbit Internship/Quality Check/Tomato Color/train'\n",
    "BASE_PATH_TEST='/home/bubbles/Quantbit Internship/Quality Check/Tomato Color/train'\n",
    "train=datasets.ImageFolder(root=BASE_PATH,transform=transform)\n",
    "test=datasets.ImageFolder(root=BASE_PATH_TEST,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5ca7d4-d085-4ec9-90d9-8cd9aa6bb269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bubbles/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader=DataLoader(train, batch_size=16,shuffle=True)\n",
    "test_loader=DataLoader(test,batch_size=16,shuffle=False)\n",
    "from torchvision import models\n",
    "model=models.vgg16('IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d025623-1642-4e80-8f81-72eabef74763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "103496e0-aa91-4e70-8f0c-c531e869c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# model.fc=nn.Linear(1000,2)\n",
    "# _modules['6']\n",
    "model.classifier._modules['6']=nn.Linear(4096,2)\n",
    "# model.classifier._modules['7']=nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c490fc4f-a697-4fcc-a7ee-f0ea4b8b044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "clf=model.to('cpu')\n",
    "opt=Adam(clf.parameters(),lr=1e-5)\n",
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044fdec8-68d9-4dd6-81ee-28978f5b1b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bubbles/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Accuracy = 67.34% Loss:0.7566224336624146\n",
      "Epoch 2: Accuracy = 80.06% Loss:0.5493375062942505\n",
      "Epoch 3: Accuracy = 84.68% Loss:0.4046052098274231\n",
      "Epoch 4: Accuracy = 90.46% Loss:0.16556815803050995\n",
      "Epoch 5: Accuracy = 92.49% Loss:0.23430418968200684\n",
      "Epoch 6: Accuracy = 94.22% Loss:0.07999013364315033\n",
      "Epoch 7: Accuracy = 95.66% Loss:0.15751847624778748\n",
      "Epoch 8: Accuracy = 96.82% Loss:0.030979808419942856\n",
      "Epoch 9: Accuracy = 97.69% Loss:0.03028394654393196\n",
      "Epoch 10: Accuracy = 97.69% Loss:0.15956397354602814\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for epoch in range(10):\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  for batch in train_loader:\n",
    "    X,y=batch\n",
    "    X,y=X.to('cpu'),y.to('cpu')\n",
    "    yhat=clf(X)\n",
    "    # print(yhat,y.unsqueeze(1))\n",
    "    # print(yhat.squeeze(1).float(),y)\n",
    "    loss=loss_fn(yhat,y)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # print(y,yhat)\n",
    "    _, predicted = torch.max(yhat, 1)\n",
    "\n",
    "        # Update the running total of correct predictions and samples\n",
    "    total_correct += (predicted == y).sum().item()\n",
    "    total_samples += y.size(0)\n",
    "\n",
    "    # Calculate the accuracy for this epoch\n",
    "  accuracy = 100 * total_correct / total_samples\n",
    "  print(f'Epoch {epoch+1}: Accuracy = {accuracy:.2f}% Loss:{loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f34c69-d786-4e66-8286-102dd3c1456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images=images.to('cpu')\n",
    "        labels=labels.to('cpu')\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = clf(images.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e41760c0-9175-4c62-b04c-67976575f16d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() missing 1 required positional argument: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTomatoClassification.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: save() missing 1 required positional argument: 'f'"
     ]
    }
   ],
   "source": [
    "torch.save(\"TomatoClassification.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d145d4d0-3470-4c89-8e90-c4f19d328c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/bubbles/Quantbit Internship/Quality Check/Tomato Color/TomatoClassification.pth\"\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "404b0386-cd16-40ef-8f4f-c0a5281b792d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VGG:\n\tsize mismatch for classifier.6.weight: copying a param with shape torch.Size([2, 4096]) from checkpoint, the shape in current model is torch.Size([1000, 4096]).\n\tsize mismatch for classifier.6.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1000]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create an instance of your model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mvgg16(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIMAGENET1K_V1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Replace with the actual class of your model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Define the image transformation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VGG:\n\tsize mismatch for classifier.6.weight: copying a param with shape torch.Size([2, 4096]) from checkpoint, the shape in current model is torch.Size([1000, 4096]).\n\tsize mismatch for classifier.6.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([1000])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have already defined and saved your model\n",
    "file_path = \"/home/bubbles/Quantbit Internship/Quality Check/Tomato Color/TomatoClassification.pth\"\n",
    "\n",
    "# Create an instance of your model\n",
    "model=models.vgg16('IMAGENET1K_V1') # Replace with the actual class of your model\n",
    "model.load_state_dict(torch.load(file_path))\n",
    "model.eval()\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust the size based on your model's input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load and preprocess the new image\n",
    "image_path = \"/home/bubbles/Downloads/green.jpeg\"\n",
    "image = Image.open(image_path)\n",
    "input_tensor = transform(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Assuming your model outputs class probabilities, you might want to get the predicted class\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52cd675-d561-441a-9b8a-46c7378b6135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bubbles/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/bubbles/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n",
      "Class probabilities: tensor([0.7833, 0.2167])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "\n",
    "# Assuming you have already defined and saved your model\n",
    "file_path = \"/home/bubbles/Quantbit Internship/Quality Check/Tomato Color/TomatoClassification.pth\"\n",
    "\n",
    "# Create an instance of the pre-trained VGG16 model\n",
    "model = models.vgg16(pretrained=True)  # Replace with the actual class of your model\n",
    "\n",
    "# Modify the last fully connected layer to match the number of classes in your specific task\n",
    "num_classes = 2  # Replace with the actual number of classes in your task\n",
    "model.classifier[6] = torch.nn.Linear(4096, num_classes)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(file_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Provide the path to the image you want to classify\n",
    "image_path = \"/home/bubbles/Downloads/tomato1.jpeg\"\n",
    "image = Image.open(image_path)\n",
    "input_tensor = transform(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Assuming your model outputs class probabilities, you might want to get the predicted class\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Class probabilities: {probabilities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888863e-0c0a-4b3d-a6d6-9f54e858a519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
