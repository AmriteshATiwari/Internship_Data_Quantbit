{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fe7f25-7a1b-4056-96eb-5005f0bf0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Resize(size=(224, 224))])\n",
    "from torchvision import datasets\n",
    "BASE_PATH=\"/home/bubbles/Quantbit Internship/Quality Check/Tomato Freshness Dataset/train\"\n",
    "BASE_PATH_TEST=\"/home/bubbles/Quantbit Internship/Quality Check/Tomato Freshness Dataset/test\"\n",
    "train=datasets.ImageFolder(root=BASE_PATH,transform=transform)\n",
    "test=datasets.ImageFolder(root=BASE_PATH_TEST,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f1af95-bd6d-4d05-8482-6d6af8e20395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bubbles/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader=DataLoader(train, batch_size=16,shuffle=True)\n",
    "test_loader=DataLoader(test,batch_size=16,shuffle=False)\n",
    "from torchvision import models\n",
    "model=models.vgg16('IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e582df66-5800-4e7c-a5ce-b59808a336b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193c1a4d-ef2e-498c-a737-5d55d26c927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# model.fc=nn.Linear(1000,2)\n",
    "# _modules['6']\n",
    "model.classifier._modules['6']=nn.Linear(4096,2)\n",
    "# model.classifier._modules['7']=nn.Sigmoid()\n",
    "from torch.optim import Adam\n",
    "clf=model.to('cpu')\n",
    "opt=Adam(clf.parameters(),lr=1e-5)\n",
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2b3c9-698c-491d-982a-986d4d6852e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bubbles/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Accuracy = 79.95% Loss:0.24163714051246643\n",
      "Epoch 2: Accuracy = 93.25% Loss:0.008190997876226902\n",
      "Epoch 3: Accuracy = 96.57% Loss:0.00865416880697012\n",
      "Epoch 4: Accuracy = 97.20% Loss:0.017823010683059692\n",
      "Epoch 5: Accuracy = 98.21% Loss:0.0036098994314670563\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for epoch in range(10):\n",
    "  total_correct = 0\n",
    "  total_samples = 0\n",
    "  for batch in train_loader:\n",
    "    X,y=batch\n",
    "    X,y=X.to('cpu'),y.to('cpu')\n",
    "    yhat=clf(X)\n",
    "    # print(yhat,y.unsqueeze(1))\n",
    "    # print(yhat.squeeze(1).float(),y)\n",
    "    loss=loss_fn(yhat,y)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # print(y,yhat)\n",
    "    _, predicted = torch.max(yhat, 1)\n",
    "\n",
    "        # Update the running total of correct predictions and samples\n",
    "    total_correct += (predicted == y).sum().item()\n",
    "    total_samples += y.size(0)\n",
    "\n",
    "    # Calculate the accuracy for this epoch\n",
    "  accuracy = 100 * total_correct / total_samples\n",
    "  print(f'Epoch {epoch+1}: Accuracy = {accuracy:.2f}% Loss:{loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f34951-e23e-4ed3-bd6e-0eec6faca16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images=images.to('cpu')\n",
    "        labels=labels.to('cpu')\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = clf(images.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8830db91-f3c8-4bf4-bf9e-f4010c2c58c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTomatofreshness.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"Tomatofreshness.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983eaba1-b64c-4720-b599-de8484002f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_json = model.to_json()\n",
    "with open(\"model3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model3.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model1 = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model1.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c713797e-7ade-4367-9900-2bfcd66440bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "#model = load_model(\"../input/final-model-saved/fruit_n_veg_model.h5\",compile=False)\n",
    "import json\n",
    "lab = training_set.class_indices\n",
    "lab={k:v for v,k in lab.items()}\n",
    "print(lab)\n",
    "print(json.dumps(lab, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a31ac-0f5e-4d03-a9e0-6289255a6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "def output(location):\n",
    "    img=load_img(location,target_size=(224,224,3))\n",
    "    plt.imshow(img)\n",
    "    img=img_to_array(img)\n",
    "    \n",
    "    img=img/255\n",
    "    plt.imshow(img)\n",
    "    img=np.expand_dims(img,[0])\n",
    "    \n",
    "    answer=model.predict(img)\n",
    "  #  ans2 = imagenet_utils.decode_predictions(answer)\n",
    "    print(answer)\n",
    "    y_class = answer.argmax(axis=-1)\n",
    "    #print(answer.argmax(axis=-))\n",
    "    y = \" \".join(str(x) for x in y_class)\n",
    "    y = int(y)\n",
    "    res = lab[y]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52d4d3-9580-4b81-98f9-950ef00c1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img=\"../input/fresh-and-stale-images-of-fruits-and-vegetables/fresh_apple/Screen Shot 2018-06-08 at 5.10.29 PM.png\"\n",
    "pic=image.load_img(img,target_size=(224,224,3))\n",
    "#plt.imshow(pic)\n",
    "output(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5536b715-8fef-413d-a8e9-3bd355cb3fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddc6ea62-86de-4970-8319-4307869d8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc82f34-289c-4652-97d7-93a1c262cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(model, input_image):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "    return predicted_class.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94542dae-9217-4357-b329-293c9e5c27ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to your image:  /home/bubbles/Downloads/Tomato2.jpeg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomato is classified as good.\n"
     ]
    }
   ],
   "source": [
    "user_image_path = input(\"Enter the path to your image: \")\n",
    "user_input_image = preprocess_image(user_image_path)\n",
    "predicted_class = predict_class(model, user_input_image)\n",
    "\n",
    "#print(f\"The predicted class for the given image is: {predicted_class}\")\n",
    "if predicted_class == 1:\n",
    "    print(\"Tomato is classified as good.\")\n",
    "else:\n",
    "    print(\"Tomato is classified as bad.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af50e8-aa65-490f-a1d2-82807bc0b986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
